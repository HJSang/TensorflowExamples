# Tensroflow Notes
This doc is summarized from [Tensorflow API docs](https://www.tensorflow.org/api_docs/python/), so that we can understand the details and do quick search and revisit.

## [autograph](https://www.tensorflow.org/api_docs/python/tf/autograph)
* Note that, in Tensorflow 2.0, AutoGraph is automatically applied when using ``tf.function``. 
* ``tf.autograph.set_verbosity``:
  * Sets the AutoGraph verbosity level.
  ```
  tf.autograph.set_verbosity(
    level, alsologtostdout=False
  )
  ```
  * Debug logging in AutoGrap
  * here are two means to control the logging verbosity:
    * The set_verbosity function
    * The AUTOGRAPH_VERBOSITY environment variable
  * ``level``: int, the verbosity level; larger values specify increased verbosity; 0 means no logging. When reporting bugs, it is recommended to set this value to a larger number, like 10.
  * ``alsologtostdout``: bool, whether to also output log messages to ``sys.stdout``
* ``tf.autograph.to_code``: Returns the source code generated by AutoGraph, as a string.
```python
def f(x):
  if x < 0:
    x = -x
  return x
print(tf.autograph.to_code(f))
```
```
def tf__f(x):
    do_return = False
    retval_ = ag__.UndefinedReturnValue()
    with ag__.FunctionScope('f', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:

        def get_state():
            return ()

        def set_state(loop_vars):
            pass

        def if_true():
            (x_1,) = (x,)
            x_1 = (- x_1)
            return x_1

        def if_false():
            return x
        cond = (x < 0)
        x = ag__.if_stmt(cond, if_true, if_false, get_state, set_state, ('x',), ())
        try:
            do_return = True
            retval_ = fscope.mark_return_value(x)
        except:
            do_return = False
            raise
    (do_return,)
    return ag__.retval(retval_)
```
* ``tf.autograph.to_graph``: Converts a Python entity into a TensorFlow graph
```python
converted_f = tf.autograph.to_graph(f)
x = tf.constant(-2)
converted_f(x)
```
```
<tf.Tensor: shape=(), dtype=int32, numpy=2>
```
* ``tf.autograph.trace``: Traces argument information at compilation time.
```python
for i in tf.range(10):
  tf.autograph.trace(i)
```
```
tf.Tensor(0, shape=(), dtype=int32)
tf.Tensor(1, shape=(), dtype=int32)
tf.Tensor(2, shape=(), dtype=int32)
tf.Tensor(3, shape=(), dtype=int32)
tf.Tensor(4, shape=(), dtype=int32)
tf.Tensor(5, shape=(), dtype=int32)
tf.Tensor(6, shape=(), dtype=int32)
tf.Tensor(7, shape=(), dtype=int32)
tf.Tensor(8, shape=(), dtype=int32)
tf.Tensor(9, shape=(), dtype=int32)
```
* Better performance with ``tf.function``
  * In TF2, eager execution is turned on by default. The user interface is intuitive and flexible but this can come at the expense of performance and deployability.
  * To get performance and portable models, use ``tf.function`` to make graphs out of your programs. However, there are pitfalls to be wary of - ``tf.function`` is not a mahical make-it-faster bullet! 
  * The main takeaways and recommendations are:
    * Debug in Eager mode, then decorate with ``@tf.function``.
    *  Don't rely on Python side effects like object mutation or list appends.
    * tf.function works best with Tensorflow ops; Numpy and Python calls are converted to constants.
  * A ``tf.function`` you define is just like a core Tensorflow operation: You can execute it eagerly; you can compute gradients and so on.
  ```python
  @tf.function
  def add(a,b):
    return a + b
  add(tf.ones([2,2]), tf.ones([2,2]))
  ```
  ```
  <tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[2., 2.],
       [2., 2.]], dtype=float32)>
  ```
  ```python
  v = tf.Variable(1.0)
  with tf.GradientTape() as tape:
    result = add(v,1.0)
  tape.gradient(result,v)
  ```
  ```
  <tf.Tensor: shape=(), dtype=float32, numpy=1.0>
  ```
* Functions can be faster than eager code, especially for graphs with many small ops. But for graphs with a few expensive ops (like convolutions), you may not see much speedup.
```python
import timeit
conv_layer = tf.keras.layers.Conv2D(100,3)

@tf.function
def conv_fn(image):
  return conv_layer(image)

image = tf.zeros([1,200,200,100])
# warm up
conv_layer(image)
conv_fn(image)

print("Eager conv:", timeit.timeit(lambda: conv_layer(image), number=10))
print("Function conv:", timeit.timeit(lambda: conv_fn(image), number=10))
```
```
Eager conv: 1.1003354969998327
Function conv: 1.0377882540001337
Note how there's not much difference in performance for convolutions
``` 
* Debugging
  * In general, debugging is easier in Eager mode than inside a ``tf.function``. You should ensure that your code executes error-free in Eager mode before decorating with ``tf.function``. To assist in the debugging process, you can call ``tf.config.run_functions_eagerly(True)`` to globally disable and reenable ``tf.function``.
  * Here are some tips:
    * Plain olf Python ``print`` calls only execute during tracing, helping you track down whne your functions get (re)traced.
    * ``tf.print`` calls will execute every time, and can help you track down intermediate values during execution.
    * ``tf.debugging.enable_check_numerics`` is an easy way to track down where NaNs and Inf are created.
    * ``pdb`` can help you understand what's going on during tracing.

* Tracing and polymorphism
  * Python's dynamic typing means that you can call functions with a variety of argument types and Python will do something different in each scenario.
  * On the other hand, Tensorflow graphs require static dtypes and shape dimensions. ``tf.function`` bridges this gap by retracing the function when necessary to generate the correct graphs. Most of the subtlety of ``tf.function`` usage stems from this retracing behavior.
```python 
# Functions are polymorphic

@tf.function
def double(a):
  print("Tracing with", a)
  return a + a

print(double(tf.constant(1)))
print()
print(double(tf.constant(1.1)))
print()
print(double(tf.constant("a")))
print()
```
```
Tracing with Tensor("a:0", shape=(), dtype=int32)
tf.Tensor(2, shape=(), dtype=int32)

Tracing with Tensor("a:0", shape=(), dtype=float32)
tf.Tensor(2.2, shape=(), dtype=float32)

Tracing with Tensor("a:0", shape=(), dtype=string)
tf.Tensor(b'aa', shape=(), dtype=string)
```
* Create a new tf.function. Separate tf.function objects are guaranteed not to share traces
* Python or Tensor args?
  * Python arguments are often used to control hyperparameters and graph constructions - for example, ``num_layers=10``. So if the Python argument changes, it makes sense that you'd have to rerace the graph.
  * However, it's possible that a Python argument is not being used to control graph construction. In these cases, a change in the Python value can trigger needless retracing. Take, for example, this training loop, which AUtoGraph will dynamically unroll. Despite the multiple traces, the generated graph is actually identical, so this is a bit inefficient.
  ```python
  def train_one_step():
    pass
  @tf.function
  def train(num_steps):
    print("Tracing with num_steps = {}".format(num_steps))
    for _ in tf.range(num_steps):
      train_one_step()
  train(num_steps=10)
  train(num_steps=20)
  ```
  * The simple workaround here is to cast your arguments to Tensors if they do not affect the shape of the generated graph.
  ```python
  train(num_steps=tf.constant(10))
  train(num_steps=tf.constant(20))
  ```
* Side effects in ``tf.function``
  * The general rule of thumb is to only use Python side effects to debug your traces.
  * Otherwise, TensorFlow ops like tf.Variable.assign, tf.print, and tf.summary are the best way to ensure your code will be traced and executed by the TensorFlow runtime with each call
  * In general using a functional style will yield the best results.
  ```python
  @tf.function
  def f(x):
    print("Traced with", x)
    tf.print("Executed with", x)
  f(1)
  f(1)
  f(2)
  ```
  ```
  Traced with 1
  Executed with 1
  Executed with 1
  Traced with 2
  Executed with 2
  ```
  * In general, while these constructs work as expected in Eager mode, many unexpected things can happen inside a tf.function due to tracing behavior
  * To give one example, advancing iterator state is a Python side effect and therefore only happens during tracing
  ```python
  external_var = tf.Variable(0)
  @tf.function
  def buggy_consume_next(iterator):
    extenal_var.assign_add(next(iterator))
    tf.print("Value of external_var:", external_var)
  
  iterator = iter([0,1,2,3])
  buggy_consume_next(iterator)
  # This reuses the first value from the iterator, rather than consuming the next value
  buggy_consume_next(iterator)
  buggy_consume_next(iterator)
  ```
  ```
  Value of external_var: 0
  Value of external_var: 0
  Value of external_var: 0
  ```
* Variables
  * We can use the same idea of leveraging the intended execution order of the code to make variable creation and utilization very easy in tf.function. There is one very important caveat, though, which is that with variables it's possible to write code which behaves differently in eager mode and graph mode.
  * Specifically, this will happen when you create a new Variable with each call. Due to tracing semantics, tf.function will reuse the same variable each call, but eager mode will create a new variable with each call. To guard against this mistake, tf.function will raise an error if it detects dangerous variable creation behavior.
  ```python
  @tf.function
  def f(x):
    v = tf.Variable(1.0)
    v.assign_add(x)
    return v

  with assert_raises(ValueError):
    f(1.0)
  ```
  ```python
  v = tf.Variable(1.0)

  @tf.function
  def f(x):
    return v.assign_add(x)

  print(f(1.0))  # 2.0
  print(f(2.0))  # 4.0
  ```

* AutoGraph Transformations
  * AutoGraph is a library that is on by default in tf.function, and transforms a subset of Python Eager code into graph-compatible TensorFlow ops. This includes control flow like if, for, while.
  * TensorFlow ops like tf.cond and tf.while_loop continue to work, but control flow is often easier to write and understand when written in Python.
  * AutoGraph will convert some if <condition> statements into the equivalent tf.cond calls. This substitution is made if <condition> is a Tensor. Otherwise, the if statement is executed as a Python conditional.
  * AutoGraph will convert some for and while statements into the equivalent TensorFlow looping ops, like tf.while_loop. If not converted, the for or while loop is executed as a Python loop.
 

 

