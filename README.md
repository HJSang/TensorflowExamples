# Tensroflow Notes
This doc is summarized from [Tensorflow API docs](https://www.tensorflow.org/api_docs/python/), so that we can understand the details and do quick search and revisit.

## [autograph](https://www.tensorflow.org/api_docs/python/tf/autograph)
* Note that, in Tensorflow 2.0, AutoGraph is automatically applied when using ``tf.function``. 
* ``tf.autograph.set_verbosity``:
  * Sets the AutoGraph verbosity level.
  ```
  tf.autograph.set_verbosity(
    level, alsologtostdout=False
  )
  ```
  * Debug logging in AutoGrap
  * here are two means to control the logging verbosity:
    * The set_verbosity function
    * The AUTOGRAPH_VERBOSITY environment variable
  * ``level``: int, the verbosity level; larger values specify increased verbosity; 0 means no logging. When reporting bugs, it is recommended to set this value to a larger number, like 10.
  * ``alsologtostdout``: bool, whether to also output log messages to ``sys.stdout``
* ``tf.autograph.to_code``: Returns the source code generated by AutoGraph, as a string.
```python
def f(x):
  if x < 0:
    x = -x
  return x
print(tf.autograph.to_code(f))
```
```
def tf__f(x):
    do_return = False
    retval_ = ag__.UndefinedReturnValue()
    with ag__.FunctionScope('f', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:

        def get_state():
            return ()

        def set_state(loop_vars):
            pass

        def if_true():
            (x_1,) = (x,)
            x_1 = (- x_1)
            return x_1

        def if_false():
            return x
        cond = (x < 0)
        x = ag__.if_stmt(cond, if_true, if_false, get_state, set_state, ('x',), ())
        try:
            do_return = True
            retval_ = fscope.mark_return_value(x)
        except:
            do_return = False
            raise
    (do_return,)
    return ag__.retval(retval_)
```
* ``tf.autograph.to_graph``: Converts a Python entity into a TensorFlow graph
```python
converted_f = tf.autograph.to_graph(f)
x = tf.constant(-2)
converted_f(x)
```
```
<tf.Tensor: shape=(), dtype=int32, numpy=2>
```
* ``tf.autograph.trace``: Traces argument information at compilation time.
```python
for i in tf.range(10):
  tf.autograph.trace(i)
```
```
tf.Tensor(0, shape=(), dtype=int32)
tf.Tensor(1, shape=(), dtype=int32)
tf.Tensor(2, shape=(), dtype=int32)
tf.Tensor(3, shape=(), dtype=int32)
tf.Tensor(4, shape=(), dtype=int32)
tf.Tensor(5, shape=(), dtype=int32)
tf.Tensor(6, shape=(), dtype=int32)
tf.Tensor(7, shape=(), dtype=int32)
tf.Tensor(8, shape=(), dtype=int32)
tf.Tensor(9, shape=(), dtype=int32)
```
* Better performance with ``tf.function``
  * In TF2, eager execution is turned on by default. The user interface is intuitive and flexible but this can come at the expense of performance and deployability.
  * To get performance and portable models, use ``tf.function`` to make graphs out of your programs. However, there are pitfalls to be wary of - ``tf.function`` is not a mahical make-it-faster bullet! 
  * The main takeaways and recommendations are:
    * Debug in Eager mode, then decorate with ``@tf.function``.
    *  Don't rely on Python side effects like object mutation or list appends.
    * tf.function works best with Tensorflow ops; Numpy and Python calls are converted to constants.
  * A ``tf.function`` you define is just like a core Tensorflow operation: You can execute it eagerly; you can compute gradients and so on.
  ```python
  @tf.function
  def add(a,b):
    return a + b
  add(tf.ones([2,2]), tf.ones([2,2]))
  ```
  ```
  <tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[2., 2.],
       [2., 2.]], dtype=float32)>
  ```
  ```python
  v = tf.Variable(1.0)
  with tf.GradientTape() as tape:
    result = add(v,1.0)
  tape.gradient(result,v)
  ```
  ```
  <tf.Tensor: shape=(), dtype=float32, numpy=1.0>
  ```
* Functions can be faster than eager code, especially for graphs with many small ops. But for graphs with a few expensive ops (like convolutions), you may not see much speedup.
```python
import timeit
conv_layer = tf.keras.layers.Conv2D(100,3)

@tf.function
def conv_fn(image):
  return conv_layer(image)

image = tf.zeros([1,200,200,100])
# warm up
conv_layer(image)
conv_fn(image)

print("Eager conv:", timeit.timeit(lambda: conv_layer(image), number=10))
print("Function conv:", timeit.timeit(lambda: conv_fn(image), number=10))
```
```
Eager conv: 1.1003354969998327
Function conv: 1.0377882540001337
Note how there's not much difference in performance for convolutions
``` 
* Debugging
  * In general, debugging is easier in Eager mode than inside a ``tf.function``. You should ensure that your code executes error-free in Eager mode before decorating with ``tf.function``. To assist in the debugging process, you can call ``tf.config.run_functions_eagerly(True)`` to globally disable and reenable ``tf.function``.
  * Here are some tips:
    * Plain olf Python ``print`` calls only execute during tracing, helping you track down whne your functions get (re)traced.
    * ``tf.print`` calls will execute every time, and can help you track down intermediate values during execution.
    * ``tf.debugging.enable_check_numerics`` is an easy way to track down where NaNs and Inf are created.
    * ``pdb`` can help you understand what's going on during tracing.

* Tracing and polymorphism
  * Python's dynamic typing means that you can call functions with a variety of argument types and Python will do something different in each scenario.
  * On the other hand, Tensorflow graphs require static dtypes and shape dimensions. ``tf.function`` bridges this gap by retracing the function when necessary to generate the correct graphs. Most of the subtlety of ``tf.function`` usage stems from this retracing behavior.
```python 
# Functions are polymorphic

@tf.function
def double(a):
  print("Tracing with", a)
  return a + a

print(double(tf.constant(1)))
print()
print(double(tf.constant(1.1)))
print()
print(double(tf.constant("a")))
print()
```
```
Tracing with Tensor("a:0", shape=(), dtype=int32)
tf.Tensor(2, shape=(), dtype=int32)

Tracing with Tensor("a:0", shape=(), dtype=float32)
tf.Tensor(2.2, shape=(), dtype=float32)

Tracing with Tensor("a:0", shape=(), dtype=string)
tf.Tensor(b'aa', shape=(), dtype=string)
```
* Create a new tf.function. Separate tf.function objects are guaranteed not to share traces

 

